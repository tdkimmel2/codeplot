DataSetInfo              : [dataset] : Added class "Signal"
                         : Add Tree pi0tree of type Signal with 2054980 events
DataSetInfo              : [dataset] : Added class "Background"
                         : Add Tree pi0tree of type Background with 4933019 events
Model: "sequential_1"
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
dense_1 (Dense)              (None, 32)                448       
_________________________________________________________________
dropout_1 (Dropout)          (None, 32)                0         
_________________________________________________________________
dense_2 (Dense)              (None, 32)                1056      
_________________________________________________________________
dropout_2 (Dropout)          (None, 32)                0         
_________________________________________________________________
dense_3 (Dense)              (None, 2)                 66        
=================================================================
Total params: 1,570
Trainable params: 1,570
Non-trainable params: 0
_________________________________________________________________
Factory                  : Booking method: [1mPyKeras[0m
                         : 
PyKeras                  : [dataset] : Create Transformation "G" with events from all classes.
                         : 
                         : Transformation, Variable selection : 
                         : Input : variable 'pi0p3' <---> Output : variable 'pi0p3'
                         : Input : variable 'pi0p3cms' <---> Output : variable 'pi0p3cms'
                         : Input : variable 'gm1e' <---> Output : variable 'gm1e'
                         : Input : variable 'gm2e' <---> Output : variable 'gm2e'
                         : Input : variable 'gm1e925' <---> Output : variable 'gm1e925'
                         : Input : variable 'gm2e925' <---> Output : variable 'gm2e925'
                         : Input : variable 'ediff' <---> Output : variable 'ediff'
                         : Input : variable 'gm1eerror' <---> Output : variable 'gm1eerror'
                         : Input : variable 'gm2eerror' <---> Output : variable 'gm2eerror'
                         : Input : variable 'gm1p3cms' <---> Output : variable 'gm1p3cms'
                         : Input : variable 'gm2p3cms' <---> Output : variable 'gm2p3cms'
                         : Input : variable 'gmthetacms' <---> Output : variable 'gmthetacms'
                         : Input : variable 'mfchi2' <---> Output : variable 'mfchi2'
                         : Load model from file: model.h5
Factory                  : Booking method: [1mGTB[0m
                         : 
Factory                  : Booking method: [1mBDT[0m
                         : 
DataSetFactory           : [dataset] : Number of events in input trees
                         : 
                         : 
                         : Number of training and testing events
                         : ---------------------------------------------------------------------------
                         : Signal     -- training events            : 10000
                         : Signal     -- testing events             : 10000
                         : Signal     -- training and testing events: 20000
                         : Background -- training events            : 50000
                         : Background -- testing events             : 50000
                         : Background -- training and testing events: 100000
                         : 
DataSetInfo              : Correlation matrix (Signal):
                         : -----------------------------------------------------------------------------------------------------------------------------
                         :               pi0p3 pi0p3cms    gm1e    gm2e gm1e925 gm2e925   ediff gm1eerror gm2eerror gm1p3cms gm2p3cms gmthetacms  mfchi2
                         :      pi0p3:  +1.000   +0.976  +0.960  +0.763  -0.140  -0.140  +0.745    +0.769    +0.629   +0.941   +0.761     -0.686  -0.186
                         :   pi0p3cms:  +0.976   +1.000  +0.937  +0.745  -0.128  -0.128  +0.728    +0.771    +0.632   +0.963   +0.782     -0.678  -0.167
                         :       gm1e:  +0.960   +0.937  +1.000  +0.553  -0.111  -0.111  +0.902    +0.828    +0.455   +0.978   +0.559     -0.619  -0.169
                         :       gm2e:  +0.763   +0.745  +0.553  +1.000  -0.163  -0.163  +0.139    +0.395    +0.833   +0.549   +0.977     -0.569  -0.185
                         :    gm1e925:  -0.140   -0.128  -0.111  -0.163  +1.000  +1.000  -0.048    -0.082    -0.161   -0.103   -0.149     +0.061  -0.045
                         :    gm2e925:  -0.140   -0.128  -0.111  -0.163  +1.000  +1.000  -0.048    -0.082    -0.161   -0.103   -0.149     +0.061  -0.045
                         :      ediff:  +0.745   +0.728  +0.902  +0.139  -0.048  -0.048  +1.000    +0.780    +0.110   +0.878   +0.159     -0.440  -0.104
                         :  gm1eerror:  +0.769   +0.771  +0.828  +0.395  -0.082  -0.082  +0.780    +1.000    +0.370   +0.830   +0.411     -0.368  -0.107
                         :  gm2eerror:  +0.629   +0.632  +0.455  +0.833  -0.161  -0.161  +0.110    +0.370    +1.000   +0.465   +0.833     -0.362  -0.119
                         :   gm1p3cms:  +0.941   +0.963  +0.978  +0.549  -0.103  -0.103  +0.878    +0.830    +0.465   +1.000   +0.587     -0.616  -0.153
                         :   gm2p3cms:  +0.761   +0.782  +0.559  +0.977  -0.149  -0.149  +0.159    +0.411    +0.833   +0.587   +1.000     -0.576  -0.170
                         : gmthetacms:  -0.686   -0.678  -0.619  -0.569  +0.061  +0.061  -0.440    -0.368    -0.362   -0.616   -0.576     +1.000  +0.171
                         :     mfchi2:  -0.186   -0.167  -0.169  -0.185  -0.045  -0.045  -0.104    -0.107    -0.119   -0.153   -0.170     +0.171  +1.000
                         : -----------------------------------------------------------------------------------------------------------------------------
DataSetInfo              : Correlation matrix (Background):
                         : -----------------------------------------------------------------------------------------------------------------------------
                         :               pi0p3 pi0p3cms    gm1e    gm2e gm1e925 gm2e925   ediff gm1eerror gm2eerror gm1p3cms gm2p3cms gmthetacms  mfchi2
                         :      pi0p3:  +1.000   +0.977  +0.968  +0.627  -0.082  -0.082  +0.858    +0.777    +0.503   +0.950   +0.627     -0.693  -0.138
                         :   pi0p3cms:  +0.977   +1.000  +0.948  +0.611  -0.078  -0.078  +0.841    +0.782    +0.506   +0.971   +0.653     -0.700  -0.128
                         :       gm1e:  +0.968   +0.948  +1.000  +0.428  -0.081  -0.081  +0.957    +0.835    +0.349   +0.980   +0.439     -0.599  -0.132
                         :       gm2e:  +0.627   +0.611  +0.428  +1.000  -0.047  -0.047  +0.146    +0.279    +0.814   +0.424   +0.963     -0.527  -0.129
                         :    gm1e925:  -0.082   -0.078  -0.081  -0.047  +1.000  +1.000  -0.073    -0.052    -0.041   -0.077   -0.048     +0.057  +0.012
                         :    gm2e925:  -0.082   -0.078  -0.081  -0.047  +1.000  +1.000  -0.073    -0.052    -0.041   -0.077   -0.048     +0.057  +0.012
                         :      ediff:  +0.858   +0.841  +0.957  +0.146  -0.073  -0.073  +1.000    +0.825    +0.120   +0.936   +0.171     -0.486  -0.102
                         :  gm1eerror:  +0.777   +0.782  +0.835  +0.279  -0.052  -0.052  +0.825    +1.000    +0.264   +0.837   +0.295     -0.344  -0.098
                         :  gm2eerror:  +0.503   +0.506  +0.349  +0.814  -0.041  -0.041  +0.120    +0.264    +1.000   +0.355   +0.804     -0.318  -0.099
                         :   gm1p3cms:  +0.950   +0.971  +0.980  +0.424  -0.077  -0.077  +0.936    +0.837    +0.355   +1.000   +0.461     -0.610  -0.123
                         :   gm2p3cms:  +0.627   +0.653  +0.439  +0.963  -0.048  -0.048  +0.171    +0.295    +0.804   +0.461   +1.000     -0.573  -0.116
                         : gmthetacms:  -0.693   -0.700  -0.599  -0.527  +0.057  +0.057  -0.486    -0.344    -0.318   -0.610   -0.573     +1.000  +0.084
                         :     mfchi2:  -0.138   -0.128  -0.132  -0.129  +0.012  +0.012  -0.102    -0.098    -0.099   -0.123   -0.116     +0.084  +1.000
                         : -----------------------------------------------------------------------------------------------------------------------------
DataSetFactory           : [dataset] :  
                         : 
Factory                  : [1mTrain all methods[0m
Factory                  : [dataset] : Create Transformation "I" with events from all classes.
                         : 
                         : Transformation, Variable selection : 
                         : Input : variable 'pi0p3' <---> Output : variable 'pi0p3'
                         : Input : variable 'pi0p3cms' <---> Output : variable 'pi0p3cms'
                         : Input : variable 'gm1e' <---> Output : variable 'gm1e'
                         : Input : variable 'gm2e' <---> Output : variable 'gm2e'
                         : Input : variable 'gm1e925' <---> Output : variable 'gm1e925'
                         : Input : variable 'gm2e925' <---> Output : variable 'gm2e925'
                         : Input : variable 'ediff' <---> Output : variable 'ediff'
                         : Input : variable 'gm1eerror' <---> Output : variable 'gm1eerror'
                         : Input : variable 'gm2eerror' <---> Output : variable 'gm2eerror'
                         : Input : variable 'gm1p3cms' <---> Output : variable 'gm1p3cms'
                         : Input : variable 'gm2p3cms' <---> Output : variable 'gm2p3cms'
                         : Input : variable 'gmthetacms' <---> Output : variable 'gmthetacms'
                         : Input : variable 'mfchi2' <---> Output : variable 'mfchi2'
Factory                  : [dataset] : Create Transformation "G" with events from all classes.
                         : 
                         : Transformation, Variable selection : 
                         : Input : variable 'pi0p3' <---> Output : variable 'pi0p3'
                         : Input : variable 'pi0p3cms' <---> Output : variable 'pi0p3cms'
                         : Input : variable 'gm1e' <---> Output : variable 'gm1e'
                         : Input : variable 'gm2e' <---> Output : variable 'gm2e'
                         : Input : variable 'gm1e925' <---> Output : variable 'gm1e925'
                         : Input : variable 'gm2e925' <---> Output : variable 'gm2e925'
                         : Input : variable 'ediff' <---> Output : variable 'ediff'
                         : Input : variable 'gm1eerror' <---> Output : variable 'gm1eerror'
                         : Input : variable 'gm2eerror' <---> Output : variable 'gm2eerror'
                         : Input : variable 'gm1p3cms' <---> Output : variable 'gm1p3cms'
                         : Input : variable 'gm2p3cms' <---> Output : variable 'gm2p3cms'
                         : Input : variable 'gmthetacms' <---> Output : variable 'gmthetacms'
                         : Input : variable 'mfchi2' <---> Output : variable 'mfchi2'
                         : Preparing the Gaussian transformation...
TFHandler_Factory        :   Variable          Mean          RMS   [        Min          Max ]
                         : ---------------------------------------------------------------------
                         :      pi0p3:   0.0034090     0.99800   [     -3.3140      5.7307 ]
                         :   pi0p3cms:   0.0033838     0.99783   [     -3.3152      5.7307 ]
                         :       gm1e:   0.0033352     0.99773   [     -3.3079      5.7307 ]
                         :       gm2e:   0.0034244     0.99771   [     -3.0852      5.7307 ]
                         :    gm1e925:     0.95014      2.3145   [     -3.2667      5.7307 ]
                         :    gm2e925:     0.95014      2.3145   [     -3.2667      5.7307 ]
                         :      ediff:   0.0033116     0.99772   [     -3.1855      5.7307 ]
                         :  gm1eerror:   0.0058067     0.99127   [     -3.0114      5.7307 ]
                         :  gm2eerror:   0.0042106     0.99563   [     -2.9388      5.7307 ]
                         :   gm1p3cms:   0.0033470     0.99765   [     -3.3127      5.7307 ]
                         :   gm2p3cms:   0.0034254     0.99796   [     -3.2974      5.7307 ]
                         : gmthetacms:   0.0037097     0.99913   [     -3.3164      5.7307 ]
                         :     mfchi2:   0.0055157     0.99612   [     -2.4772      5.7307 ]
                         : ---------------------------------------------------------------------
                         : Ranking input variables (method unspecific)...
Id_GaussTransformation   : Ranking result (top variable is best ranked)
                         : -----------------------------------
                         : Rank : Variable   : Separation
                         : -----------------------------------
                         :    1 : pi0p3      : 1.906e-01
                         :    2 : mfchi2     : 1.858e-01
                         :    3 : gmthetacms : 1.701e-01
                         :    4 : gm2e       : 1.673e-01
                         :    5 : gm1e       : 1.631e-01
                         :    6 : pi0p3cms   : 1.595e-01
                         :    7 : gm2eerror  : 1.500e-01
                         :    8 : gm1eerror  : 1.495e-01
                         :    9 : gm2p3cms   : 1.388e-01
                         :   10 : gm1p3cms   : 1.381e-01
                         :   11 : ediff      : 7.501e-02
                         :   12 : gm1e925    : 5.923e-02
                         :   13 : gm2e925    : 5.923e-02
                         : -----------------------------------
Factory                  : Train method: PyKeras for Classification
                         : 
                         : 
                         : [1m================================================================[0m
                         : [1mH e l p   f o r   M V A   m e t h o d   [ PyKeras ] :[0m
                         : 
                         : Keras is a high-level API for the Theano and Tensorflow packages.
                         : This method wraps the training and predictions steps of the Keras
                         : Python package for TMVA, so that dataloading, preprocessing and
                         : evaluation can be done within the TMVA system. To use this Keras
                         : interface, you have to generate a model with Keras first. Then,
                         : this model can be loaded and trained in TMVA.
                         : 
                         : 
                         : <Suppress this message by specifying "!H" in the booking option>
                         : [1m================================================================[0m
                         : 
                         : Preparing the Gaussian transformation...
TFHandler_PyKeras        :   Variable          Mean          RMS   [        Min          Max ]
                         : ---------------------------------------------------------------------
                         :      pi0p3:   0.0034090     0.99800   [     -3.3140      5.7307 ]
                         :   pi0p3cms:   0.0033838     0.99783   [     -3.3152      5.7307 ]
                         :       gm1e:   0.0033352     0.99773   [     -3.3079      5.7307 ]
                         :       gm2e:   0.0034244     0.99771   [     -3.0852      5.7307 ]
                         :    gm1e925:     0.95014      2.3145   [     -3.2667      5.7307 ]
                         :    gm2e925:     0.95014      2.3145   [     -3.2667      5.7307 ]
                         :      ediff:   0.0033116     0.99772   [     -3.1855      5.7307 ]
                         :  gm1eerror:   0.0058067     0.99127   [     -3.0114      5.7307 ]
                         :  gm2eerror:   0.0042106     0.99563   [     -2.9388      5.7307 ]
                         :   gm1p3cms:   0.0033470     0.99765   [     -3.3127      5.7307 ]
                         :   gm2p3cms:   0.0034254     0.99796   [     -3.2974      5.7307 ]
                         : gmthetacms:   0.0037097     0.99913   [     -3.3164      5.7307 ]
                         :     mfchi2:   0.0055157     0.99612   [     -2.4772      5.7307 ]
                         : ---------------------------------------------------------------------
                         : Option SaveBestOnly: Only model weights with smallest validation loss will be stored
Train on 60000 samples, validate on 60000 samples
Epoch 1/10

   32/60000 [..............................] - ETA: 5:35 - loss: 1.2682 - categorical_accuracy: 0.4375
 1568/60000 [..............................] - ETA: 8s - loss: 0.7167 - categorical_accuracy: 0.6562  
 3168/60000 [>.............................] - ETA: 5s - loss: 0.6052 - categorical_accuracy: 0.7348
 4576/60000 [=>............................] - ETA: 4s - loss: 0.5698 - categorical_accuracy: 0.7646
 6176/60000 [==>...........................] - ETA: 3s - loss: 0.5357 - categorical_accuracy: 0.7803
 7744/60000 [==>...........................] - ETA: 2s - loss: 0.5161 - categorical_accuracy: 0.7909
 9408/60000 [===>..........................] - ETA: 2s - loss: 0.4968 - categorical_accuracy: 0.8010
10976/60000 [====>.........................] - ETA: 2s - loss: 0.4836 - categorical_accuracy: 0.8065
12544/60000 [=====>........................] - ETA: 2s - loss: 0.4742 - categorical_accuracy: 0.8106
14112/60000 [======>.......................] - ETA: 2s - loss: 0.4653 - categorical_accuracy: 0.8147
15680/60000 [======>.......................] - ETA: 1s - loss: 0.4572 - categorical_accuracy: 0.8182
17248/60000 [=======>......................] - ETA: 1s - loss: 0.4522 - categorical_accuracy: 0.8213
18848/60000 [========>.....................] - ETA: 1s - loss: 0.4464 - categorical_accuracy: 0.8234
20416/60000 [=========>....................] - ETA: 1s - loss: 0.4420 - categorical_accuracy: 0.8248
22016/60000 [==========>...................] - ETA: 1s - loss: 0.4382 - categorical_accuracy: 0.8270
23648/60000 [==========>...................] - ETA: 1s - loss: 0.4353 - categorical_accuracy: 0.8281
25248/60000 [===========>..................] - ETA: 1s - loss: 0.4314 - categorical_accuracy: 0.8292
26848/60000 [============>.................] - ETA: 1s - loss: 0.4289 - categorical_accuracy: 0.8303
28416/60000 [=============>................] - ETA: 1s - loss: 0.4262 - categorical_accuracy: 0.8307
30016/60000 [==============>...............] - ETA: 1s - loss: 0.4232 - categorical_accuracy: 0.8315
31552/60000 [==============>...............] - ETA: 1s - loss: 0.4212 - categorical_accuracy: 0.8320
33184/60000 [===============>..............] - ETA: 1s - loss: 0.4181 - categorical_accuracy: 0.8332
34816/60000 [================>.............] - ETA: 0s - loss: 0.4161 - categorical_accuracy: 0.8337
36352/60000 [=================>............] - ETA: 0s - loss: 0.4158 - categorical_accuracy: 0.8340
37984/60000 [=================>............] - ETA: 0s - loss: 0.4140 - categorical_accuracy: 0.8346
39616/60000 [==================>...........] - ETA: 0s - loss: 0.4138 - categorical_accuracy: 0.8344
41248/60000 [===================>..........] - ETA: 0s - loss: 0.4122 - categorical_accuracy: 0.8348
42848/60000 [====================>.........] - ETA: 0s - loss: 0.4106 - categorical_accuracy: 0.8353
44448/60000 [=====================>........] - ETA: 0s - loss: 0.4088 - categorical_accuracy: 0.8361
46016/60000 [======================>.......] - ETA: 0s - loss: 0.4080 - categorical_accuracy: 0.8364
47552/60000 [======================>.......] - ETA: 0s - loss: 0.4075 - categorical_accuracy: 0.8365
49120/60000 [=======================>......] - ETA: 0s - loss: 0.4067 - categorical_accuracy: 0.8365
50720/60000 [========================>.....] - ETA: 0s - loss: 0.4062 - categorical_accuracy: 0.8365
52320/60000 [=========================>....] - ETA: 0s - loss: 0.4051 - categorical_accuracy: 0.8368
53952/60000 [=========================>....] - ETA: 0s - loss: 0.4041 - categorical_accuracy: 0.8367
55584/60000 [==========================>...] - ETA: 0s - loss: 0.4030 - categorical_accuracy: 0.8371
57152/60000 [===========================>..] - ETA: 0s - loss: 0.4017 - categorical_accuracy: 0.8374
58784/60000 [============================>.] - ETA: 0s - loss: 0.4011 - categorical_accuracy: 0.8372
60000/60000 [==============================] - 3s 54us/step - loss: 0.4004 - categorical_accuracy: 0.8374 - val_loss: 0.3514 - val_categorical_accuracy: 0.8526

Epoch 00001: val_loss improved from inf to 0.35142, saving model to dataset/weights/TrainedModel_PyKeras.h5
Epoch 2/10

   32/60000 [..............................] - ETA: 5s - loss: 0.4086 - categorical_accuracy: 0.8125
 1536/60000 [..............................] - ETA: 2s - loss: 0.3585 - categorical_accuracy: 0.8574
 3200/60000 [>.............................] - ETA: 1s - loss: 0.3592 - categorical_accuracy: 0.8597
 4736/60000 [=>............................] - ETA: 1s - loss: 0.3578 - categorical_accuracy: 0.8600
 6400/60000 [==>...........................] - ETA: 1s - loss: 0.3611 - categorical_accuracy: 0.8583
 7904/60000 [==>...........................] - ETA: 1s - loss: 0.3635 - categorical_accuracy: 0.8569
 9568/60000 [===>..........................] - ETA: 1s - loss: 0.3670 - categorical_accuracy: 0.8533
11168/60000 [====>.........................] - ETA: 1s - loss: 0.3657 - categorical_accuracy: 0.8527
12608/60000 [=====>........................] - ETA: 1s - loss: 0.3658 - categorical_accuracy: 0.8527
14144/60000 [======>.......................] - ETA: 1s - loss: 0.3646 - categorical_accuracy: 0.8524
15776/60000 [======>.......................] - ETA: 1s - loss: 0.3646 - categorical_accuracy: 0.8515
17376/60000 [=======>......................] - ETA: 1s - loss: 0.3652 - categorical_accuracy: 0.8509
19040/60000 [========>.....................] - ETA: 1s - loss: 0.3655 - categorical_accuracy: 0.8504
20736/60000 [=========>....................] - ETA: 1s - loss: 0.3667 - categorical_accuracy: 0.8498
22336/60000 [==========>...................] - ETA: 1s - loss: 0.3666 - categorical_accuracy: 0.8497
23968/60000 [==========>...................] - ETA: 1s - loss: 0.3659 - categorical_accuracy: 0.8498
25600/60000 [===========>..................] - ETA: 1s - loss: 0.3637 - categorical_accuracy: 0.8507
27104/60000 [============>.................] - ETA: 1s - loss: 0.3629 - categorical_accuracy: 0.8511
28704/60000 [=============>................] - ETA: 0s - loss: 0.3630 - categorical_accuracy: 0.8517
30176/60000 [==============>...............] - ETA: 0s - loss: 0.3634 - categorical_accuracy: 0.8511
31808/60000 [==============>...............] - ETA: 0s - loss: 0.3652 - categorical_accuracy: 0.8501
33408/60000 [===============>..............] - ETA: 0s - loss: 0.3649 - categorical_accuracy: 0.8502
35008/60000 [================>.............] - ETA: 0s - loss: 0.3651 - categorical_accuracy: 0.8501
36608/60000 [=================>............] - ETA: 0s - loss: 0.3645 - categorical_accuracy: 0.8506
38176/60000 [==================>...........] - ETA: 0s - loss: 0.3646 - categorical_accuracy: 0.8511
39808/60000 [==================>...........] - ETA: 0s - loss: 0.3642 - categorical_accuracy: 0.8510
41376/60000 [===================>..........] - ETA: 0s - loss: 0.3645 - categorical_accuracy: 0.8508
42976/60000 [====================>.........] - ETA: 0s - loss: 0.3646 - categorical_accuracy: 0.8506
44608/60000 [=====================>........] - ETA: 0s - loss: 0.3642 - categorical_accuracy: 0.8505
46176/60000 [======================>.......] - ETA: 0s - loss: 0.3655 - categorical_accuracy: 0.8501
47776/60000 [======================>.......] - ETA: 0s - loss: 0.3652 - categorical_accuracy: 0.8502
49376/60000 [=======================>......] - ETA: 0s - loss: 0.3654 - categorical_accuracy: 0.8501
50976/60000 [========================>.....] - ETA: 0s - loss: 0.3650 - categorical_accuracy: 0.8501
52544/60000 [=========================>....] - ETA: 0s - loss: 0.3650 - categorical_accuracy: 0.8501
54112/60000 [==========================>...] - ETA: 0s - loss: 0.3645 - categorical_accuracy: 0.8500
55648/60000 [==========================>...] - ETA: 0s - loss: 0.3644 - categorical_accuracy: 0.8501
57216/60000 [===========================>..] - ETA: 0s - loss: 0.3645 - categorical_accuracy: 0.8501
58816/60000 [============================>.] - ETA: 0s - loss: 0.3645 - categorical_accuracy: 0.8502
60000/60000 [==============================] - 3s 50us/step - loss: 0.3650 - categorical_accuracy: 0.8498 - val_loss: 0.3476 - val_categorical_accuracy: 0.8547

Epoch 00002: val_loss improved from 0.35142 to 0.34755, saving model to dataset/weights/TrainedModel_PyKeras.h5
Epoch 3/10

   32/60000 [..............................] - ETA: 4s - loss: 0.4614 - categorical_accuracy: 0.8438
 1568/60000 [..............................] - ETA: 1s - loss: 0.3720 - categorical_accuracy: 0.8482
 3200/60000 [>.............................] - ETA: 1s - loss: 0.3776 - categorical_accuracy: 0.8462
 4672/60000 [=>............................] - ETA: 1s - loss: 0.3730 - categorical_accuracy: 0.8470
 6240/60000 [==>...........................] - ETA: 1s - loss: 0.3742 - categorical_accuracy: 0.8473
 7840/60000 [==>...........................] - ETA: 1s - loss: 0.3745 - categorical_accuracy: 0.8494
 9440/60000 [===>..........................] - ETA: 1s - loss: 0.3735 - categorical_accuracy: 0.8477
11008/60000 [====>.........................] - ETA: 1s - loss: 0.3698 - categorical_accuracy: 0.8494
12608/60000 [=====>........................] - ETA: 1s - loss: 0.3706 - categorical_accuracy: 0.8495
14208/60000 [======>.......................] - ETA: 1s - loss: 0.3662 - categorical_accuracy: 0.8514
15808/60000 [======>.......................] - ETA: 1s - loss: 0.3641 - categorical_accuracy: 0.8526
17408/60000 [=======>......................] - ETA: 1s - loss: 0.3625 - categorical_accuracy: 0.8536
18976/60000 [========>.....................] - ETA: 1s - loss: 0.3642 - categorical_accuracy: 0.8524
20576/60000 [=========>....................] - ETA: 1s - loss: 0.3631 - categorical_accuracy: 0.8520
22176/60000 [==========>...................] - ETA: 1s - loss: 0.3622 - categorical_accuracy: 0.8518
23776/60000 [==========>...................] - ETA: 1s - loss: 0.3617 - categorical_accuracy: 0.8523
25376/60000 [===========>..................] - ETA: 1s - loss: 0.3608 - categorical_accuracy: 0.8523
26976/60000 [============>.................] - ETA: 1s - loss: 0.3609 - categorical_accuracy: 0.8525
28480/60000 [=============>................] - ETA: 1s - loss: 0.3591 - categorical_accuracy: 0.8535
30016/60000 [==============>...............] - ETA: 0s - loss: 0.3588 - categorical_accuracy: 0.8538
31648/60000 [==============>...............] - ETA: 0s - loss: 0.3595 - categorical_accuracy: 0.8534
33280/60000 [===============>..............] - ETA: 0s - loss: 0.3582 - categorical_accuracy: 0.8540
34784/60000 [================>.............] - ETA: 0s - loss: 0.3582 - categorical_accuracy: 0.8538
36352/60000 [=================>............] - ETA: 0s - loss: 0.3587 - categorical_accuracy: 0.8539
37952/60000 [=================>............] - ETA: 0s - loss: 0.3583 - categorical_accuracy: 0.8541
39552/60000 [==================>...........] - ETA: 0s - loss: 0.3578 - categorical_accuracy: 0.8541
41152/60000 [===================>..........] - ETA: 0s - loss: 0.3582 - categorical_accuracy: 0.8535
42752/60000 [====================>.........] - ETA: 0s - loss: 0.3585 - categorical_accuracy: 0.8532
44384/60000 [=====================>........] - ETA: 0s - loss: 0.3588 - categorical_accuracy: 0.8532
45920/60000 [=====================>........] - ETA: 0s - loss: 0.3586 - categorical_accuracy: 0.8532
47552/60000 [======================>.......] - ETA: 0s - loss: 0.3585 - categorical_accuracy: 0.8532
48992/60000 [=======================>......] - ETA: 0s - loss: 0.3579 - categorical_accuracy: 0.8534
50656/60000 [========================>.....] - ETA: 0s - loss: 0.3575 - categorical_accuracy: 0.8537
52224/60000 [=========================>....] - ETA: 0s - loss: 0.3571 - categorical_accuracy: 0.8538
53792/60000 [=========================>....] - ETA: 0s - loss: 0.3572 - categorical_accuracy: 0.8536
55392/60000 [==========================>...] - ETA: 0s - loss: 0.3574 - categorical_accuracy: 0.8535
56992/60000 [===========================>..] - ETA: 0s - loss: 0.3582 - categorical_accuracy: 0.8533
58560/60000 [============================>.] - ETA: 0s - loss: 0.3589 - categorical_accuracy: 0.8529
60000/60000 [==============================] - 3s 50us/step - loss: 0.3594 - categorical_accuracy: 0.8524 - val_loss: 0.3474 - val_categorical_accuracy: 0.8532

Epoch 00003: val_loss improved from 0.34755 to 0.34739, saving model to dataset/weights/TrainedModel_PyKeras.h5
Epoch 4/10

   32/60000 [..............................] - ETA: 5s - loss: 0.2955 - categorical_accuracy: 0.8438
 1600/60000 [..............................] - ETA: 1s - loss: 0.3664 - categorical_accuracy: 0.8512
 3136/60000 [>.............................] - ETA: 1s - loss: 0.3684 - categorical_accuracy: 0.8460
 4736/60000 [=>............................] - ETA: 1s - loss: 0.3599 - categorical_accuracy: 0.8511
 6336/60000 [==>...........................] - ETA: 1s - loss: 0.3572 - categorical_accuracy: 0.8545
 7968/60000 [==>...........................] - ETA: 1s - loss: 0.3553 - categorical_accuracy: 0.8530
 9536/60000 [===>..........................] - ETA: 1s - loss: 0.3605 - categorical_accuracy: 0.8505
11200/60000 [====>.........................] - ETA: 1s - loss: 0.3579 - categorical_accuracy: 0.8522
12736/60000 [=====>........................] - ETA: 1s - loss: 0.3565 - categorical_accuracy: 0.8518
14368/60000 [======>.......................] - ETA: 1s - loss: 0.3540 - categorical_accuracy: 0.8534
15904/60000 [======>.......................] - ETA: 1s - loss: 0.3550 - categorical_accuracy: 0.8528
17568/60000 [=======>......................] - ETA: 1s - loss: 0.3548 - categorical_accuracy: 0.8526
19008/60000 [========>.....................] - ETA: 1s - loss: 0.3533 - categorical_accuracy: 0.8536
20640/60000 [=========>....................] - ETA: 1s - loss: 0.3542 - categorical_accuracy: 0.8542
22176/60000 [==========>...................] - ETA: 1s - loss: 0.3539 - categorical_accuracy: 0.8543
23840/60000 [==========>...................] - ETA: 1s - loss: 0.3538 - categorical_accuracy: 0.8544
25408/60000 [===========>..................] - ETA: 1s - loss: 0.3549 - categorical_accuracy: 0.8538
27008/60000 [============>.................] - ETA: 1s - loss: 0.3535 - categorical_accuracy: 0.8545
28576/60000 [=============>................] - ETA: 1s - loss: 0.3539 - categorical_accuracy: 0.8544
30208/60000 [==============>...............] - ETA: 0s - loss: 0.3545 - categorical_accuracy: 0.8537
31872/60000 [==============>...............] - ETA: 0s - loss: 0.3541 - categorical_accuracy: 0.8538
33280/60000 [===============>..............] - ETA: 0s - loss: 0.3527 - categorical_accuracy: 0.8544
34912/60000 [================>.............] - ETA: 0s - loss: 0.3536 - categorical_accuracy: 0.8540
36480/60000 [=================>............] - ETA: 0s - loss: 0.3551 - categorical_accuracy: 0.8537
38144/60000 [==================>...........] - ETA: 0s - loss: 0.3558 - categorical_accuracy: 0.8530
39680/60000 [==================>...........] - ETA: 0s - loss: 0.3563 - categorical_accuracy: 0.8524
41312/60000 [===================>..........] - ETA: 0s - loss: 0.3568 - categorical_accuracy: 0.8524
42848/60000 [====================>.........] - ETA: 0s - loss: 0.3576 - categorical_accuracy: 0.8514
44480/60000 [=====================>........] - ETA: 0s - loss: 0.3568 - categorical_accuracy: 0.8515
46016/60000 [======================>.......] - ETA: 0s - loss: 0.3567 - categorical_accuracy: 0.8517
47648/60000 [======================>.......] - ETA: 0s - loss: 0.3565 - categorical_accuracy: 0.8517
49248/60000 [=======================>......] - ETA: 0s - loss: 0.3570 - categorical_accuracy: 0.8515
50848/60000 [========================>.....] - ETA: 0s - loss: 0.3570 - categorical_accuracy: 0.8514
52352/60000 [=========================>....] - ETA: 0s - loss: 0.3575 - categorical_accuracy: 0.8514
53984/60000 [=========================>....] - ETA: 0s - loss: 0.3569 - categorical_accuracy: 0.8516
55520/60000 [==========================>...] - ETA: 0s - loss: 0.3562 - categorical_accuracy: 0.8519
57184/60000 [===========================>..] - ETA: 0s - loss: 0.3563 - categorical_accuracy: 0.8520
58688/60000 [============================>.] - ETA: 0s - loss: 0.3566 - categorical_accuracy: 0.8521
60000/60000 [==============================] - 3s 50us/step - loss: 0.3566 - categorical_accuracy: 0.8520 - val_loss: 0.3437 - val_categorical_accuracy: 0.8558

Epoch 00004: val_loss improved from 0.34739 to 0.34368, saving model to dataset/weights/TrainedModel_PyKeras.h5
Epoch 5/10

   32/60000 [..............................] - ETA: 4s - loss: 0.2369 - categorical_accuracy: 0.9375
 1248/60000 [..............................] - ETA: 2s - loss: 0.3774 - categorical_accuracy: 0.8446
 2432/60000 [>.............................] - ETA: 2s - loss: 0.3648 - categorical_accuracy: 0.8483
 3712/60000 [>.............................] - ETA: 2s - loss: 0.3563 - categorical_accuracy: 0.8575
 5216/60000 [=>............................] - ETA: 2s - loss: 0.3525 - categorical_accuracy: 0.8560
 6848/60000 [==>...........................] - ETA: 1s - loss: 0.3598 - categorical_accuracy: 0.8537
 8448/60000 [===>..........................] - ETA: 1s - loss: 0.3586 - categorical_accuracy: 0.8543
10016/60000 [====>.........................] - ETA: 1s - loss: 0.3567 - categorical_accuracy: 0.8546
11616/60000 [====>.........................] - ETA: 1s - loss: 0.3590 - categorical_accuracy: 0.8526
13248/60000 [=====>........................] - ETA: 1s - loss: 0.3616 - categorical_accuracy: 0.8510
14848/60000 [======>.......................] - ETA: 1s - loss: 0.3622 - categorical_accuracy: 0.8512
16448/60000 [=======>......................] - ETA: 1s - loss: 0.3618 - categorical_accuracy: 0.8508
18048/60000 [========>.....................] - ETA: 1s - loss: 0.3602 - categorical_accuracy: 0.8513
19616/60000 [========>.....................] - ETA: 1s - loss: 0.3605 - categorical_accuracy: 0.8508
21184/60000 [=========>....................] - ETA: 1s - loss: 0.3617 - categorical_accuracy: 0.8505
22816/60000 [==========>...................] - ETA: 1s - loss: 0.3606 - categorical_accuracy: 0.8513
24384/60000 [===========>..................] - ETA: 1s - loss: 0.3599 - categorical_accuracy: 0.8514
25920/60000 [===========>..................] - ETA: 1s - loss: 0.3599 - categorical_accuracy: 0.8517
27456/60000 [============>.................] - ETA: 1s - loss: 0.3602 - categorical_accuracy: 0.8516
29120/60000 [=============>................] - ETA: 1s - loss: 0.3603 - categorical_accuracy: 0.8517
30720/60000 [==============>...............] - ETA: 0s - loss: 0.3602 - categorical_accuracy: 0.8516
32256/60000 [===============>..............] - ETA: 0s - loss: 0.3597 - categorical_accuracy: 0.8521
33792/60000 [===============>..............] - ETA: 0s - loss: 0.3589 - categorical_accuracy: 0.8526
35424/60000 [================>.............] - ETA: 0s - loss: 0.3583 - categorical_accuracy: 0.8528
36992/60000 [=================>............] - ETA: 0s - loss: 0.3589 - categorical_accuracy: 0.8525
38656/60000 [==================>...........] - ETA: 0s - loss: 0.3589 - categorical_accuracy: 0.8526
40256/60000 [===================>..........] - ETA: 0s - loss: 0.3585 - categorical_accuracy: 0.8529
41888/60000 [===================>..........] - ETA: 0s - loss: 0.3577 - categorical_accuracy: 0.8534
43456/60000 [====================>.........] - ETA: 0s - loss: 0.3569 - categorical_accuracy: 0.8537
45120/60000 [=====================>........] - ETA: 0s - loss: 0.3562 - categorical_accuracy: 0.8539
46720/60000 [======================>.......] - ETA: 0s - loss: 0.3557 - categorical_accuracy: 0.8541
48384/60000 [=======================>......] - ETA: 0s - loss: 0.3551 - categorical_accuracy: 0.8542
49920/60000 [=======================>......] - ETA: 0s - loss: 0.3552 - categorical_accuracy: 0.8541
51584/60000 [========================>.....] - ETA: 0s - loss: 0.3554 - categorical_accuracy: 0.8539
53152/60000 [=========================>....] - ETA: 0s - loss: 0.3551 - categorical_accuracy: 0.8539
54784/60000 [==========================>...] - ETA: 0s - loss: 0.3548 - categorical_accuracy: 0.8543
56352/60000 [===========================>..] - ETA: 0s - loss: 0.3545 - categorical_accuracy: 0.8543
57824/60000 [===========================>..] - ETA: 0s - loss: 0.3552 - categorical_accuracy: 0.8538
59392/60000 [============================>.] - ETA: 0s - loss: 0.3551 - categorical_accuracy: 0.8538
60000/60000 [==============================] - 3s 51us/step - loss: 0.3551 - categorical_accuracy: 0.8537 - val_loss: 0.3452 - val_categorical_accuracy: 0.8536

Epoch 00005: val_loss did not improve from 0.34368
Epoch 6/10

   32/60000 [..............................] - ETA: 4s - loss: 0.2938 - categorical_accuracy: 0.8750
 1632/60000 [..............................] - ETA: 1s - loss: 0.3720 - categorical_accuracy: 0.8517
 3264/60000 [>.............................] - ETA: 1s - loss: 0.3621 - categorical_accuracy: 0.8505
 4832/60000 [=>............................] - ETA: 1s - loss: 0.3572 - categorical_accuracy: 0.8526
 6464/60000 [==>...........................] - ETA: 1s - loss: 0.3586 - categorical_accuracy: 0.8507
 8032/60000 [===>..........................] - ETA: 1s - loss: 0.3584 - categorical_accuracy: 0.8510
 9536/60000 [===>..........................] - ETA: 1s - loss: 0.3568 - categorical_accuracy: 0.8514
11136/60000 [====>.........................] - ETA: 1s - loss: 0.3538 - categorical_accuracy: 0.8529
12768/60000 [=====>........................] - ETA: 1s - loss: 0.3560 - categorical_accuracy: 0.8505
14336/60000 [======>.......................] - ETA: 1s - loss: 0.3553 - categorical_accuracy: 0.8504
15968/60000 [======>.......................] - ETA: 1s - loss: 0.3559 - categorical_accuracy: 0.8503
17504/60000 [=======>......................] - ETA: 1s - loss: 0.3565 - categorical_accuracy: 0.8502
19168/60000 [========>.....................] - ETA: 1s - loss: 0.3556 - categorical_accuracy: 0.8504
20736/60000 [=========>....................] - ETA: 1s - loss: 0.3552 - categorical_accuracy: 0.8512
22400/60000 [==========>...................] - ETA: 1s - loss: 0.3531 - categorical_accuracy: 0.8522
23968/60000 [==========>...................] - ETA: 1s - loss: 0.3539 - categorical_accuracy: 0.8516
25536/60000 [===========>..................] - ETA: 1s - loss: 0.3553 - categorical_accuracy: 0.8515
27104/60000 [============>.................] - ETA: 1s - loss: 0.3545 - categorical_accuracy: 0.8520
28736/60000 [=============>................] - ETA: 0s - loss: 0.3558 - categorical_accuracy: 0.8513
30336/60000 [==============>...............] - ETA: 0s - loss: 0.3552 - categorical_accuracy: 0.8514
32000/60000 [===============>..............] - ETA: 0s - loss: 0.3538 - categorical_accuracy: 0.8523
33568/60000 [===============>..............] - ETA: 0s - loss: 0.3536 - categorical_accuracy: 0.8529
35200/60000 [================>.............] - ETA: 0s - loss: 0.3531 - categorical_accuracy: 0.8536
36800/60000 [=================>............] - ETA: 0s - loss: 0.3529 - categorical_accuracy: 0.8534
38464/60000 [==================>...........] - ETA: 0s - loss: 0.3531 - categorical_accuracy: 0.8535
40000/60000 [===================>..........] - ETA: 0s - loss: 0.3529 - categorical_accuracy: 0.8534
41632/60000 [===================>..........] - ETA: 0s - loss: 0.3529 - categorical_accuracy: 0.8535
43232/60000 [====================>.........] - ETA: 0s - loss: 0.3526 - categorical_accuracy: 0.8539
44896/60000 [=====================>........] - ETA: 0s - loss: 0.3532 - categorical_accuracy: 0.8539
46496/60000 [======================>.......] - ETA: 0s - loss: 0.3531 - categorical_accuracy: 0.8541
48128/60000 [=======================>......] - ETA: 0s - loss: 0.3529 - categorical_accuracy: 0.8542
49696/60000 [=======================>......] - ETA: 0s - loss: 0.3527 - categorical_accuracy: 0.8542
51360/60000 [========================>.....] - ETA: 0s - loss: 0.3532 - categorical_accuracy: 0.8540
52960/60000 [=========================>....] - ETA: 0s - loss: 0.3534 - categorical_accuracy: 0.8541
54528/60000 [==========================>...] - ETA: 0s - loss: 0.3533 - categorical_accuracy: 0.8542
56096/60000 [===========================>..] - ETA: 0s - loss: 0.3538 - categorical_accuracy: 0.8539
57632/60000 [===========================>..] - ETA: 0s - loss: 0.3536 - categorical_accuracy: 0.8539
59200/60000 [============================>.] - ETA: 0s - loss: 0.3537 - categorical_accuracy: 0.8539
60000/60000 [==============================] - 3s 50us/step - loss: 0.3543 - categorical_accuracy: 0.8537 - val_loss: 0.3433 - val_categorical_accuracy: 0.8597

Epoch 00006: val_loss improved from 0.34368 to 0.34329, saving model to dataset/weights/TrainedModel_PyKeras.h5
Epoch 7/10

   32/60000 [..............................] - ETA: 4s - loss: 0.2691 - categorical_accuracy: 0.9062
 1536/60000 [..............................] - ETA: 2s - loss: 0.3552 - categorical_accuracy: 0.8542
 3136/60000 [>.............................] - ETA: 1s - loss: 0.3548 - categorical_accuracy: 0.8504
 4704/60000 [=>............................] - ETA: 1s - loss: 0.3533 - categorical_accuracy: 0.8561
 6304/60000 [==>...........................] - ETA: 1s - loss: 0.3473 - categorical_accuracy: 0.8588
 7904/60000 [==>...........................] - ETA: 1s - loss: 0.3458 - categorical_accuracy: 0.8598
 9504/60000 [===>..........................] - ETA: 1s - loss: 0.3464 - categorical_accuracy: 0.8592
11136/60000 [====>.........................] - ETA: 1s - loss: 0.3509 - categorical_accuracy: 0.8571
12736/60000 [=====>........................] - ETA: 1s - loss: 0.3549 - categorical_accuracy: 0.8552
14336/60000 [======>.......................] - ETA: 1s - loss: 0.3546 - categorical_accuracy: 0.8544
15936/60000 [======>.......................] - ETA: 1s - loss: 0.3563 - categorical_accuracy: 0.8540
17536/60000 [=======>......................] - ETA: 1s - loss: 0.3538 - categorical_accuracy: 0.8557
19072/60000 [========>.....................] - ETA: 1s - loss: 0.3530 - categorical_accuracy: 0.8558
20672/60000 [=========>....................] - ETA: 1s - loss: 0.3531 - categorical_accuracy: 0.8562
22272/60000 [==========>...................] - ETA: 1s - loss: 0.3541 - categorical_accuracy: 0.8555
23712/60000 [==========>...................] - ETA: 1s - loss: 0.3569 - categorical_accuracy: 0.8537
25152/60000 [===========>..................] - ETA: 1s - loss: 0.3571 - categorical_accuracy: 0.8537
26752/60000 [============>.................] - ETA: 1s - loss: 0.3571 - categorical_accuracy: 0.8538
28416/60000 [=============>................] - ETA: 1s - loss: 0.3571 - categorical_accuracy: 0.8530
29984/60000 [=============>................] - ETA: 0s - loss: 0.3563 - categorical_accuracy: 0.8531
31584/60000 [==============>...............] - ETA: 0s - loss: 0.3559 - categorical_accuracy: 0.8531
33184/60000 [===============>..............] - ETA: 0s - loss: 0.3553 - categorical_accuracy: 0.8536
34752/60000 [================>.............] - ETA: 0s - loss: 0.3551 - categorical_accuracy: 0.8533
36384/60000 [=================>............] - ETA: 0s - loss: 0.3555 - categorical_accuracy: 0.8534
37952/60000 [=================>............] - ETA: 0s - loss: 0.3552 - categorical_accuracy: 0.8535
39616/60000 [==================>...........] - ETA: 0s - loss: 0.3549 - categorical_accuracy: 0.8539
40928/60000 [===================>..........] - ETA: 0s - loss: 0.3553 - categorical_accuracy: 0.8538
42560/60000 [====================>.........] - ETA: 0s - loss: 0.3554 - categorical_accuracy: 0.8538
44128/60000 [=====================>........] - ETA: 0s - loss: 0.3554 - categorical_accuracy: 0.8537
45760/60000 [=====================>........] - ETA: 0s - loss: 0.3554 - categorical_accuracy: 0.8535
47328/60000 [======================>.......] - ETA: 0s - loss: 0.3555 - categorical_accuracy: 0.8534
48896/60000 [=======================>......] - ETA: 0s - loss: 0.3559 - categorical_accuracy: 0.8529
50528/60000 [========================>.....] - ETA: 0s - loss: 0.3548 - categorical_accuracy: 0.8533
52160/60000 [=========================>....] - ETA: 0s - loss: 0.3538 - categorical_accuracy: 0.8538
53728/60000 [=========================>....] - ETA: 0s - loss: 0.3536 - categorical_accuracy: 0.8539
55360/60000 [==========================>...] - ETA: 0s - loss: 0.3531 - categorical_accuracy: 0.8542
56896/60000 [===========================>..] - ETA: 0s - loss: 0.3540 - categorical_accuracy: 0.8537
58496/60000 [============================>.] - ETA: 0s - loss: 0.3536 - categorical_accuracy: 0.8538
60000/60000 [==============================] - 3s 50us/step - loss: 0.3539 - categorical_accuracy: 0.8537 - val_loss: 0.3426 - val_categorical_accuracy: 0.8590

Epoch 00007: val_loss improved from 0.34329 to 0.34260, saving model to dataset/weights/TrainedModel_PyKeras.h5
Epoch 8/10

   32/60000 [..............................] - ETA: 6s - loss: 0.2486 - categorical_accuracy: 0.9688
 1568/60000 [..............................] - ETA: 2s - loss: 0.3393 - categorical_accuracy: 0.8597
 3104/60000 [>.............................] - ETA: 1s - loss: 0.3480 - categorical_accuracy: 0.8579
 4672/60000 [=>............................] - ETA: 1s - loss: 0.3488 - categorical_accuracy: 0.8581
 6304/60000 [==>...........................] - ETA: 1s - loss: 0.3499 - categorical_accuracy: 0.8591
 7904/60000 [==>...........................] - ETA: 1s - loss: 0.3531 - categorical_accuracy: 0.8575
 9504/60000 [===>..........................] - ETA: 1s - loss: 0.3562 - categorical_accuracy: 0.8555
11104/60000 [====>.........................] - ETA: 1s - loss: 0.3576 - categorical_accuracy: 0.8547
12672/60000 [=====>........................] - ETA: 1s - loss: 0.3560 - categorical_accuracy: 0.8551
14240/60000 [======>.......................] - ETA: 1s - loss: 0.3559 - categorical_accuracy: 0.8543
15840/60000 [======>.......................] - ETA: 1s - loss: 0.3566 - categorical_accuracy: 0.8537
17376/60000 [=======>......................] - ETA: 1s - loss: 0.3560 - categorical_accuracy: 0.8542
18976/60000 [========>.....................] - ETA: 1s - loss: 0.3571 - categorical_accuracy: 0.8543
20608/60000 [=========>....................] - ETA: 1s - loss: 0.3571 - categorical_accuracy: 0.8543
22240/60000 [==========>...................] - ETA: 1s - loss: 0.3578 - categorical_accuracy: 0.8536
23872/60000 [==========>...................] - ETA: 1s - loss: 0.3589 - categorical_accuracy: 0.8523
25280/60000 [===========>..................] - ETA: 1s - loss: 0.3592 - categorical_accuracy: 0.8520
26848/60000 [============>.................] - ETA: 1s - loss: 0.3581 - categorical_accuracy: 0.8522
28480/60000 [=============>................] - ETA: 1s - loss: 0.3586 - categorical_accuracy: 0.8522
30112/60000 [==============>...............] - ETA: 0s - loss: 0.3580 - categorical_accuracy: 0.8529
31712/60000 [==============>...............] - ETA: 0s - loss: 0.3572 - categorical_accuracy: 0.8533
33216/60000 [===============>..............] - ETA: 0s - loss: 0.3565 - categorical_accuracy: 0.8534
34784/60000 [================>.............] - ETA: 0s - loss: 0.3555 - categorical_accuracy: 0.8538
36416/60000 [=================>............] - ETA: 0s - loss: 0.3553 - categorical_accuracy: 0.8539
38016/60000 [==================>...........] - ETA: 0s - loss: 0.3544 - categorical_accuracy: 0.8542
39648/60000 [==================>...........] - ETA: 0s - loss: 0.3537 - categorical_accuracy: 0.8546
41248/60000 [===================>..........] - ETA: 0s - loss: 0.3534 - categorical_accuracy: 0.8546
42848/60000 [====================>.........] - ETA: 0s - loss: 0.3537 - categorical_accuracy: 0.8543
44448/60000 [=====================>........] - ETA: 0s - loss: 0.3548 - categorical_accuracy: 0.8538
46112/60000 [======================>.......] - ETA: 0s - loss: 0.3549 - categorical_accuracy: 0.8538
47584/60000 [======================>.......] - ETA: 0s - loss: 0.3550 - categorical_accuracy: 0.8538
49248/60000 [=======================>......] - ETA: 0s - loss: 0.3552 - categorical_accuracy: 0.8539
50784/60000 [========================>.....] - ETA: 0s - loss: 0.3549 - categorical_accuracy: 0.8541
52416/60000 [=========================>....] - ETA: 0s - loss: 0.3549 - categorical_accuracy: 0.8539
53952/60000 [=========================>....] - ETA: 0s - loss: 0.3558 - categorical_accuracy: 0.8533
55488/60000 [==========================>...] - ETA: 0s - loss: 0.3552 - categorical_accuracy: 0.8534
57024/60000 [===========================>..] - ETA: 0s - loss: 0.3547 - categorical_accuracy: 0.8537
58656/60000 [============================>.] - ETA: 0s - loss: 0.3544 - categorical_accuracy: 0.8537
60000/60000 [==============================] - 3s 50us/step - loss: 0.3542 - categorical_accuracy: 0.8538 - val_loss: 0.3409 - val_categorical_accuracy: 0.8583

Epoch 00008: val_loss improved from 0.34260 to 0.34095, saving model to dataset/weights/TrainedModel_PyKeras.h5
Epoch 9/10

   32/60000 [..............................] - ETA: 5s - loss: 0.3761 - categorical_accuracy: 0.8750
 1568/60000 [..............................] - ETA: 1s - loss: 0.3514 - categorical_accuracy: 0.8578
 3168/60000 [>.............................] - ETA: 1s - loss: 0.3459 - categorical_accuracy: 0.8564
 4768/60000 [=>............................] - ETA: 1s - loss: 0.3476 - categorical_accuracy: 0.8551
 6336/60000 [==>...........................] - ETA: 1s - loss: 0.3535 - categorical_accuracy: 0.8531
 7904/60000 [==>...........................] - ETA: 1s - loss: 0.3504 - categorical_accuracy: 0.8551
 9568/60000 [===>..........................] - ETA: 1s - loss: 0.3504 - categorical_accuracy: 0.8559
11136/60000 [====>.........................] - ETA: 1s - loss: 0.3443 - categorical_accuracy: 0.8604
12800/60000 [=====>........................] - ETA: 1s - loss: 0.3460 - categorical_accuracy: 0.8596
14400/60000 [======>.......................] - ETA: 1s - loss: 0.3467 - categorical_accuracy: 0.8592
16000/60000 [=======>......................] - ETA: 1s - loss: 0.3473 - categorical_accuracy: 0.8599
17504/60000 [=======>......................] - ETA: 1s - loss: 0.3492 - categorical_accuracy: 0.8579
19104/60000 [========>.....................] - ETA: 1s - loss: 0.3502 - categorical_accuracy: 0.8576
20704/60000 [=========>....................] - ETA: 1s - loss: 0.3498 - categorical_accuracy: 0.8576
22368/60000 [==========>...................] - ETA: 1s - loss: 0.3491 - categorical_accuracy: 0.8577
23712/60000 [==========>...................] - ETA: 1s - loss: 0.3495 - categorical_accuracy: 0.8567
25376/60000 [===========>..................] - ETA: 1s - loss: 0.3489 - categorical_accuracy: 0.8569
26944/60000 [============>.................] - ETA: 1s - loss: 0.3467 - categorical_accuracy: 0.8580
28608/60000 [=============>................] - ETA: 1s - loss: 0.3480 - categorical_accuracy: 0.8571
30176/60000 [==============>...............] - ETA: 0s - loss: 0.3484 - categorical_accuracy: 0.8567
31712/60000 [==============>...............] - ETA: 0s - loss: 0.3495 - categorical_accuracy: 0.8560
33280/60000 [===============>..............] - ETA: 0s - loss: 0.3497 - categorical_accuracy: 0.8561
34912/60000 [================>.............] - ETA: 0s - loss: 0.3503 - categorical_accuracy: 0.8559
36512/60000 [=================>............] - ETA: 0s - loss: 0.3493 - categorical_accuracy: 0.8564
38144/60000 [==================>...........] - ETA: 0s - loss: 0.3497 - categorical_accuracy: 0.8561
39712/60000 [==================>...........] - ETA: 0s - loss: 0.3495 - categorical_accuracy: 0.8562
41344/60000 [===================>..........] - ETA: 0s - loss: 0.3493 - categorical_accuracy: 0.8561
42912/60000 [====================>.........] - ETA: 0s - loss: 0.3499 - categorical_accuracy: 0.8557
44576/60000 [=====================>........] - ETA: 0s - loss: 0.3495 - categorical_accuracy: 0.8557
46080/60000 [======================>.......] - ETA: 0s - loss: 0.3500 - categorical_accuracy: 0.8556
47712/60000 [======================>.......] - ETA: 0s - loss: 0.3507 - categorical_accuracy: 0.8556
49216/60000 [=======================>......] - ETA: 0s - loss: 0.3508 - categorical_accuracy: 0.8555
50848/60000 [========================>.....] - ETA: 0s - loss: 0.3504 - categorical_accuracy: 0.8555
52352/60000 [=========================>....] - ETA: 0s - loss: 0.3503 - categorical_accuracy: 0.8555
53984/60000 [=========================>....] - ETA: 0s - loss: 0.3504 - categorical_accuracy: 0.8555
55520/60000 [==========================>...] - ETA: 0s - loss: 0.3506 - categorical_accuracy: 0.8555
57120/60000 [===========================>..] - ETA: 0s - loss: 0.3509 - categorical_accuracy: 0.8553
58720/60000 [============================>.] - ETA: 0s - loss: 0.3515 - categorical_accuracy: 0.8550
60000/60000 [==============================] - 3s 50us/step - loss: 0.3519 - categorical_accuracy: 0.8548 - val_loss: 0.3428 - val_categorical_accuracy: 0.8589

Epoch 00009: val_loss did not improve from 0.34095
Epoch 10/10

   32/60000 [..............................] - ETA: 4s - loss: 0.3606 - categorical_accuracy: 0.8750
 1632/60000 [..............................] - ETA: 1s - loss: 0.3691 - categorical_accuracy: 0.8468
 3232/60000 [>.............................] - ETA: 1s - loss: 0.3520 - categorical_accuracy: 0.8549
 4832/60000 [=>............................] - ETA: 1s - loss: 0.3534 - categorical_accuracy: 0.8533
 6464/60000 [==>...........................] - ETA: 1s - loss: 0.3548 - categorical_accuracy: 0.8509
 8032/60000 [===>..........................] - ETA: 1s - loss: 0.3564 - categorical_accuracy: 0.8506
 9696/60000 [===>..........................] - ETA: 1s - loss: 0.3557 - categorical_accuracy: 0.8510
10880/60000 [====>.........................] - ETA: 1s - loss: 0.3537 - categorical_accuracy: 0.8515
12512/60000 [=====>........................] - ETA: 1s - loss: 0.3551 - categorical_accuracy: 0.8500
14048/60000 [======>.......................] - ETA: 1s - loss: 0.3552 - categorical_accuracy: 0.8503
15680/60000 [======>.......................] - ETA: 1s - loss: 0.3579 - categorical_accuracy: 0.8487
17216/60000 [=======>......................] - ETA: 1s - loss: 0.3583 - categorical_accuracy: 0.8487
18880/60000 [========>.....................] - ETA: 1s - loss: 0.3567 - categorical_accuracy: 0.8502
20448/60000 [=========>....................] - ETA: 1s - loss: 0.3579 - categorical_accuracy: 0.8498
22048/60000 [==========>...................] - ETA: 1s - loss: 0.3565 - categorical_accuracy: 0.8506
23648/60000 [==========>...................] - ETA: 1s - loss: 0.3551 - categorical_accuracy: 0.8514
25184/60000 [===========>..................] - ETA: 1s - loss: 0.3545 - categorical_accuracy: 0.8515
26816/60000 [============>.................] - ETA: 1s - loss: 0.3532 - categorical_accuracy: 0.8521
28448/60000 [=============>................] - ETA: 1s - loss: 0.3529 - categorical_accuracy: 0.8522
30112/60000 [==============>...............] - ETA: 0s - loss: 0.3529 - categorical_accuracy: 0.8527
31680/60000 [==============>...............] - ETA: 0s - loss: 0.3531 - categorical_accuracy: 0.8526
33312/60000 [===============>..............] - ETA: 0s - loss: 0.3546 - categorical_accuracy: 0.8518
34912/60000 [================>.............] - ETA: 0s - loss: 0.3544 - categorical_accuracy: 0.8519
36608/60000 [=================>............] - ETA: 0s - loss: 0.3545 - categorical_accuracy: 0.8517
38176/60000 [==================>...........] - ETA: 0s - loss: 0.3543 - categorical_accuracy: 0.8520
39776/60000 [==================>...........] - ETA: 0s - loss: 0.3547 - categorical_accuracy: 0.8519
41312/60000 [===================>..........] - ETA: 0s - loss: 0.3541 - categorical_accuracy: 0.8523
42912/60000 [====================>.........] - ETA: 0s - loss: 0.3541 - categorical_accuracy: 0.8525
44096/60000 [=====================>........] - ETA: 0s - loss: 0.3534 - categorical_accuracy: 0.8527
45408/60000 [=====================>........] - ETA: 0s - loss: 0.3527 - categorical_accuracy: 0.8530
46752/60000 [======================>.......] - ETA: 0s - loss: 0.3527 - categorical_accuracy: 0.8532
48416/60000 [=======================>......] - ETA: 0s - loss: 0.3523 - categorical_accuracy: 0.8533
49984/60000 [=======================>......] - ETA: 0s - loss: 0.3523 - categorical_accuracy: 0.8535
51648/60000 [========================>.....] - ETA: 0s - loss: 0.3525 - categorical_accuracy: 0.8532
53216/60000 [=========================>....] - ETA: 0s - loss: 0.3525 - categorical_accuracy: 0.8533
54720/60000 [==========================>...] - ETA: 0s - loss: 0.3526 - categorical_accuracy: 0.8529
56320/60000 [===========================>..] - ETA: 0s - loss: 0.3529 - categorical_accuracy: 0.8528
57888/60000 [===========================>..] - ETA: 0s - loss: 0.3532 - categorical_accuracy: 0.8525
59520/60000 [============================>.] - ETA: 0s - loss: 0.3529 - categorical_accuracy: 0.8527
60000/60000 [==============================] - 3s 50us/step - loss: 0.3529 - categorical_accuracy: 0.8527 - val_loss: 0.3414 - val_categorical_accuracy: 0.8573

Epoch 00010: val_loss did not improve from 0.34095
                         : Elapsed time for training with 60000 events: [1;31m31.7 sec[0m         
                         : Creating xml weight file: [0;36mdataset/weights/TMVAClassification_PyKeras.weights.xml[0m
                         : Creating standalone class: [0;36mdataset/weights/TMVAClassification_PyKeras.class.C[0m
Factory                  : Training finished
                         : 
Factory                  : Train method: GTB for Classification
                         : 
                         : 
                         : [1m================================================================[0m
                         : [1mH e l p   f o r   M V A   m e t h o d   [ GTB ] :[0m
                         : A gradient tree boosting classifier builds a model from an ensemble
                         : of decision trees, which are adapted each boosting step to fit better
                         : to previously misclassified events.
                         : 
                         : Check out the scikit-learn documentation for more information.
                         : 
                         : <Suppress this message by specifying "!H" in the booking option>
                         : [1m================================================================[0m
                         : 
                         : 
                         : [1mSaving state file: [0mdataset/weights/PyGTBModel_GTB.PyData
                         : 
                         : Elapsed time for training with 60000 events: [1;31m25 sec[0m         
                         : Creating xml weight file: [0;36mdataset/weights/TMVAClassification_GTB.weights.xml[0m
                         : Creating standalone class: [0;36mdataset/weights/TMVAClassification_GTB.class.C[0m
Factory                  : Training finished
                         : 
Factory                  : Train method: BDT for Classification
                         : 
BDT                      : #events: (reweighted) sig: 30000 bkg: 30000
                         : #events: (unweighted) sig: 10000 bkg: 50000
                         : Training 10 Decision Trees ... patience please
                         : Elapsed time for training with 60000 events: [1;31m2.08 sec[0m         
BDT                      : [dataset] : Evaluation of BDT on training sample (60000 events)
                         : Elapsed time for evaluation of 60000 events: [1;31m0.0421 sec[0m       
                         : Creating xml weight file: [0;36mdataset/weights/TMVAClassification_BDT.weights.xml[0m
                         : Creating standalone class: [0;36mdataset/weights/TMVAClassification_BDT.class.C[0m
                         : MVAOutput.root:/dataset/Method_BDT/BDT
Factory                  : Training finished
                         : 
                         : Ranking input variables (method specific)...
                         : No variable ranking supplied by classifier: PyKeras
GTB                      : Ranking result (top variable is best ranked)
                         : --------------------------------------------
                         : Rank : Variable   : Variable Importance
                         : --------------------------------------------
                         :    1 : mfchi2     : 3.026e-01
                         :    2 : pi0p3      : 2.746e-01
                         :    3 : gm2e       : 1.969e-01
                         :    4 : gm1e925    : 3.823e-02
                         :    5 : gm2p3cms   : 3.292e-02
                         :    6 : gm2e925    : 3.218e-02
                         :    7 : gmthetacms : 2.652e-02
                         :    8 : pi0p3cms   : 2.318e-02
                         :    9 : gm1p3cms   : 2.075e-02
                         :   10 : gm1e       : 1.455e-02
                         :   11 : gm1eerror  : 1.371e-02
                         :   12 : gm2eerror  : 1.261e-02
                         :   13 : ediff      : 1.134e-02
                         : --------------------------------------------
BDT                      : Ranking result (top variable is best ranked)
                         : --------------------------------------------
                         : Rank : Variable   : Variable Importance
                         : --------------------------------------------
                         :    1 : mfchi2     : 3.258e-01
                         :    2 : pi0p3      : 2.381e-01
                         :    3 : gm2e       : 2.116e-01
                         :    4 : gm1e925    : 9.300e-02
                         :    5 : gm2p3cms   : 4.981e-02
                         :    6 : gmthetacms : 2.522e-02
                         :    7 : pi0p3cms   : 2.003e-02
                         :    8 : ediff      : 1.826e-02
                         :    9 : gm1p3cms   : 1.499e-02
                         :   10 : gm2eerror  : 3.130e-03
                         :   11 : gm1e       : 0.000e+00
                         :   12 : gm2e925    : 0.000e+00
                         :   13 : gm1eerror  : 0.000e+00
                         : --------------------------------------------
Factory                  : === Destroy and recreate all methods via weight files for testing ===
                         : 
Factory                  : [1mTest all methods[0m
Factory                  : Test method: PyKeras for Classification performance
                         : 
                         : Load model from file: dataset/weights/TrainedModel_PyKeras.h5
Factory                  : Test method: GTB for Classification performance
                         : 
                         : 
                         : [1mLoading state file: [0mdataset/weights/PyGTBModel_GTB.PyData
                         : 
Factory                  : Test method: BDT for Classification performance
                         : 
BDT                      : [dataset] : Evaluation of BDT on testing sample (60000 events)
                         : Elapsed time for evaluation of 60000 events: [1;31m0.0418 sec[0m       
Factory                  : [1mEvaluate all methods[0m
Factory                  : Evaluate classifier: PyKeras
                         : 
TFHandler_PyKeras        :   Variable          Mean          RMS   [        Min          Max ]
                         : ---------------------------------------------------------------------
                         :      pi0p3:    0.014829     0.99427   [     -3.3086      5.7307 ]
                         :   pi0p3cms:    0.012604     0.99497   [     -3.4218      5.7307 ]
                         :       gm1e:    0.015251     0.99251   [     -3.3059      5.7307 ]
                         :       gm2e:    0.012561     0.99809   [     -3.0831      5.7307 ]
                         :    gm1e925:     0.93809      2.3146   [     -3.8013      5.7307 ]
                         :    gm2e925:     0.93809      2.3146   [     -3.8013      5.7307 ]
                         :      ediff:    0.012632     0.99305   [     -3.1906      5.7307 ]
                         :  gm1eerror:    0.018868     0.98611   [     -2.9839      5.7307 ]
                         :  gm2eerror:    0.013528     0.99739   [     -2.9399      5.7307 ]
                         :   gm1p3cms:    0.012897     0.99426   [     -3.3714      5.7307 ]
                         :   gm2p3cms:    0.010408     0.99994   [     -3.2881      5.7307 ]
                         : gmthetacms:  -0.0049234     0.99587   [     -3.3183      5.7307 ]
                         :     mfchi2:  -0.0013915     0.99651   [     -2.4772      5.7307 ]
                         : ---------------------------------------------------------------------
PyKeras                  : [dataset] : Loop over test events and fill histograms with classifier response...
                         : 
TFHandler_PyKeras        :   Variable          Mean          RMS   [        Min          Max ]
                         : ---------------------------------------------------------------------
                         :      pi0p3:    0.014829     0.99427   [     -3.3086      5.7307 ]
                         :   pi0p3cms:    0.012604     0.99497   [     -3.4218      5.7307 ]
                         :       gm1e:    0.015251     0.99251   [     -3.3059      5.7307 ]
                         :       gm2e:    0.012561     0.99809   [     -3.0831      5.7307 ]
                         :    gm1e925:     0.93809      2.3146   [     -3.8013      5.7307 ]
                         :    gm2e925:     0.93809      2.3146   [     -3.8013      5.7307 ]
                         :      ediff:    0.012632     0.99305   [     -3.1906      5.7307 ]
                         :  gm1eerror:    0.018868     0.98611   [     -2.9839      5.7307 ]
                         :  gm2eerror:    0.013528     0.99739   [     -2.9399      5.7307 ]
                         :   gm1p3cms:    0.012897     0.99426   [     -3.3714      5.7307 ]
                         :   gm2p3cms:    0.010408     0.99994   [     -3.2881      5.7307 ]
                         : gmthetacms:  -0.0049234     0.99587   [     -3.3183      5.7307 ]
                         :     mfchi2:  -0.0013915     0.99651   [     -2.4772      5.7307 ]
                         : ---------------------------------------------------------------------
Factory                  : Evaluate classifier: GTB
                         : 
GTB                      : [dataset] : Loop over test events and fill histograms with classifier response...
                         : 
TFHandler_GTB            :   Variable          Mean          RMS   [        Min          Max ]
                         : ---------------------------------------------------------------------
                         :      pi0p3:     0.39613     0.36474   [   0.0010822      4.7564 ]
                         :   pi0p3cms:     0.50976     0.48903   [   0.0048890      6.9169 ]
                         :       gm1e:     0.30323     0.28253   [    0.061266      4.6417 ]
                         :       gm2e:     0.12816     0.10251   [    0.060001      1.9371 ]
                         :    gm1e925:     0.94966    0.063036   [     0.18367      1.0000 ]
                         :    gm2e925:     0.94966    0.063036   [     0.18367      1.0000 ]
                         :      ediff:     0.17507     0.24346   [  3.2783e-07      4.5259 ]
                         :  gm1eerror:  6.7867e-05  0.00019296   [  1.9590e-06    0.011752 ]
                         :  gm2eerror:  1.2482e-05  3.4406e-05   [  1.6849e-06   0.0015334 ]
                         :   gm1p3cms:     0.38050     0.38320   [    0.045125      6.6191 ]
                         :   gm2p3cms:     0.15851     0.13884   [    0.042823      2.8570 ]
                         : gmthetacms:     0.90966     0.54889   [    0.041413      3.1048 ]
                         :     mfchi2:      12.182      12.580   [  1.3247e-08      49.996 ]
                         : ---------------------------------------------------------------------
Factory                  : Evaluate classifier: BDT
                         : 
BDT                      : [dataset] : Loop over test events and fill histograms with classifier response...
                         : 
TFHandler_BDT            :   Variable          Mean          RMS   [        Min          Max ]
                         : ---------------------------------------------------------------------
                         :      pi0p3:     0.39613     0.36474   [   0.0010822      4.7564 ]
                         :   pi0p3cms:     0.50976     0.48903   [   0.0048890      6.9169 ]
                         :       gm1e:     0.30323     0.28253   [    0.061266      4.6417 ]
                         :       gm2e:     0.12816     0.10251   [    0.060001      1.9371 ]
                         :    gm1e925:     0.94966    0.063036   [     0.18367      1.0000 ]
                         :    gm2e925:     0.94966    0.063036   [     0.18367      1.0000 ]
                         :      ediff:     0.17507     0.24346   [  3.2783e-07      4.5259 ]
                         :  gm1eerror:  6.7867e-05  0.00019296   [  1.9590e-06    0.011752 ]
                         :  gm2eerror:  1.2482e-05  3.4406e-05   [  1.6849e-06   0.0015334 ]
                         :   gm1p3cms:     0.38050     0.38320   [    0.045125      6.6191 ]
                         :   gm2p3cms:     0.15851     0.13884   [    0.042823      2.8570 ]
                         : gmthetacms:     0.90966     0.54889   [    0.041413      3.1048 ]
                         :     mfchi2:      12.182      12.580   [  1.3247e-08      49.996 ]
                         : ---------------------------------------------------------------------
                         : 
                         : Evaluation results ranked by best signal efficiency and purity (area)
                         : -------------------------------------------------------------------------------------------------------------------
                         : DataSet       MVA                       
                         : Name:         Method:          ROC-integ
                         : dataset       PyKeras        : 0.832
                         : dataset       GTB            : 0.832
                         : dataset       BDT            : 0.820
                         : -------------------------------------------------------------------------------------------------------------------
                         : 
                         : Testing efficiency compared to training efficiency (overtraining check)
                         : -------------------------------------------------------------------------------------------------------------------
                         : DataSet              MVA              Signal efficiency: from test sample (from training sample) 
                         : Name:                Method:          @B=0.01             @B=0.10            @B=0.30   
                         : -------------------------------------------------------------------------------------------------------------------
                         : dataset              PyKeras        : 0.185 (0.185)       0.558 (0.559)      0.799 (0.804)
                         : dataset              GTB            : 0.176 (0.246)       0.554 (0.584)      0.799 (0.812)
                         : dataset              BDT            : 0.000 (0.000)       0.523 (0.531)      0.787 (0.790)
                         : -------------------------------------------------------------------------------------------------------------------
                         : 
Dataset:dataset          : Created tree 'TestTree' with 60000 events
                         : 
Dataset:dataset          : Created tree 'TrainTree' with 60000 events
                         : 
Factory                  : [1mThank you for using TMVA![0m
                         : [1mFor citation information, please visit: http://tmva.sf.net/citeTMVA.html[0m
